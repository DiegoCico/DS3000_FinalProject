{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase II: Data Curation, Exploratory Analysis and Plotting\n",
    "## Stock Market Predictor\n",
    "\n",
    "### Names: Diego Cicotoste, Ariv Ahuja, Salma Elmosallamy, Rithik Raghuraman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "How does the stock market work? how can you predict the stock market? what tools can you use? The stock market can seem complex and unpredictable, some would even say gambeling. One of the hardest challenges is making educated or informed decisions. The goal of this project is to tackle the uncertainty and help, stock traders make better decision on wether a stock is tradable or not. Wether to buy or sell. I would use past historical trends to make educated predictions on how the stock market would react."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Retrieval**  \n",
    "- We used the **`yfinance` API** to retrieve daily historical **Open, High, Low, Close (OHLC)** prices and **volume data** for **S&P 500 stocks**, focusing on **Amazon (AMZN)** for the **past year**.\n",
    "- The retrieved data includes essential market metrics that will serve as the foundation for feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Data Cleaning and Processing**\n",
    "\n",
    "#### **Handling Missing Data**  \n",
    "- No data was missing after inspection\n",
    "\n",
    "#### **Feature Engineering: Technical Indicators**  \n",
    "We calculated several key **technical indicators** to enrich the dataset:\n",
    "  - **RSI (Relative Strength Index)**: Momentum indicator over 14 days.\n",
    "  - **VWAP (Volume Weighted Average Price)**: Measures the average trading price weighted by volume.\n",
    "  - **EMA (Exponential Moving Average)**: Captures the smoothed trend over 20 days.\n",
    "  - **ADX (Average Directional Index)**: Quantifies trend strength.\n",
    "\n",
    "#### **More Features: Sentiment Analysis from News Articles**  \n",
    "- We fetched relevant **news articles** using **NewsAPI** for the same period as the stock data.\n",
    "- **VADER Sentiment Analysis** was used to calculate **compound sentiment scores** for each article.\n",
    "- Sentiment scores were **aggregated by date** to align with the stock OHLC data.\n",
    "\n",
    "#### **Data Alignment and Merging**  \n",
    "- We ensured **alignment** between **OHLC data, technical indicators, log returns, and sentiment scores** using date-based indices.\n",
    "- The combined DataFrame was prepared, with all relevant features available for further analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Visualization of the Cleaned Data**\n",
    "\n",
    "We visualized the **cleaned and processed dataset** to understand key trends and patterns:\n",
    "\n",
    "1. **Price Trends and Indicators**:\n",
    "   - **OHLC Candlestick Plots**: Show stock price movements.\n",
    "   - **Overlaying VWAP and EMA**: To track trends and identify support/resistance levels.\n",
    "   - **RSI and ADX Line Plots**: Visualize momentum and trend strength over time.\n",
    "\n",
    "2. **Volume Analysis**:\n",
    "   - **Normalized Volume**: Visualized to detect significant changes in trading activity.\n",
    "\n",
    "3. **Sentiment Trends**:\n",
    "   - **Sentiment Score Line Chart**: Displays how public sentiment fluctuates over time.\n",
    "   - **Overlay of Sentiment with Stock Price**: To observe correlations between sentiment and price movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rithik/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_stock_data(symbol: str, period: str, interval: str = '1d') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve stock price data for a given symbol, time period, and interval.\n",
    "    Returns the stock prices as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): The ticker symbol of the stock (e.g., 'AAPL').\n",
    "        period (str): The period to retrieve data (e.g., '1y', '6mo', '5d').\n",
    "        interval (str): The data interval (e.g., '1d', '1wk', '1mo').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing historical stock prices.\n",
    "    \"\"\"\n",
    "    # Fetch data from Yahoo Finance\n",
    "    stock_data = yf.download(symbol, period=period, interval=interval)\n",
    "\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_log_returns(close: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the log returns from the close prices.\n",
    "    \n",
    "    Parameters:\n",
    "        close (np.ndarray): Array of closing prices.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of log returns.\n",
    "    \"\"\"\n",
    "    log_returns = np.log(close / close.shift(1))\n",
    "    \n",
    "    return log_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "\n",
    "def calculate_technical_indicators(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate technical indicators and return them as NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing historical stock prices.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with technical indicators as NumPy arrays.\n",
    "    \"\"\"\n",
    "    indicators = {}\n",
    "\n",
    "    # Calculate RSI (Relative Strength Index)\n",
    "    indicators['rsi'] = ta.rsi(df['Close'], length=20).to_numpy()\n",
    "\n",
    "    # Calculate 20-day Exponential Moving Average (EMA)\n",
    "    indicators['ema'] = ta.ema(df['Close'], length=20).to_numpy()\n",
    "\n",
    "    # Calculate ADX (Average Directional Index)\n",
    "    adx_df = ta.adx(df['High'], df['Low'], df['Close'], length=20)\n",
    "    indicators['adx'] = adx_df['ADX_20'].to_numpy()\n",
    "\n",
    "    # Calculate VWAP (Volume Weighted Average Price)\n",
    "    vwap_series = ta.vwap(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "    indicators['vwap'] = vwap_series.to_numpy()\n",
    "\n",
    "    # Calculate normalized volume\n",
    "    indicators['normalized_volume'] = (df['Volume'] / df['Volume'].rolling(window=20).mean()).to_numpy()\n",
    "\n",
    "    return indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "def get_seekingalpha_articles(ticker: str, start_date: datetime, end_date: datetime):\n",
    "    \"\"\"\n",
    "    Scrapes article titles and publication dates for a given stock ticker \n",
    "    from Seeking Alpha within the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "        ticker (str): Stock ticker symbol (e.g., 'AMZN').\n",
    "        start_date (datetime): Start date as a datetime object.\n",
    "        end_date (datetime): End date as a datetime object.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Series of article titles.\n",
    "        pd.Series: Series of article dates.\n",
    "    \"\"\"\n",
    "    start_str = start_date.strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    end_str = end_date.strftime('%Y-%m-%dT%H:%M:%S.999Z')\n",
    "\n",
    "    # URL-encode the formatted strings\n",
    "    start_str = quote(start_str, safe='')\n",
    "    end_str = quote(end_str, safe='')\n",
    "\n",
    "    page_num = 1\n",
    "    all_titles, all_dates = [], []\n",
    "\n",
    "\n",
    "    # Set up Chrome options for headless mode\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # Initialize undetected-chromedriver with the configured options\n",
    "    driver = uc.Chrome()\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://seekingalpha.com/symbol/{ticker}/news?from={start_str}&page={page_num}&to={end_str}\"\n",
    "\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        driver.get(url)\n",
    "    \n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(3)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Extract titles and dates\n",
    "        titles = soup.find_all('a', {'data-test-id': 'post-list-item-title'})\n",
    "        dates = soup.find_all('span', {'data-test-id': 'post-list-date'})\n",
    "\n",
    "        if not titles or not dates:\n",
    "            break  # Stop if no more articles are found\n",
    "\n",
    "        for title, date in zip(titles, dates):\n",
    "            title_text = title.text.strip()\n",
    "            date_str = date.text.strip()\n",
    "\n",
    "            # Handle if it is recent\n",
    "            if date_str.lower().startswith('yest'):\n",
    "                parsed_date = datetime.now() - timedelta(days=1)\n",
    "            elif date_str.lower().startswith('toda'):\n",
    "                parsed_date = datetime.now()\n",
    "                \n",
    "            # Handle date strings with or without a year\n",
    "            elif date_str[-4:].isdigit():  # If the string contains a year\n",
    "                date_format = '%a, %b %d, %Y' if \"May\" in date_str else '%a, %b. %d, %Y'\n",
    "                parsed_date = datetime.strptime(date_str, date_format)\n",
    "            else:\n",
    "                date_str = f'{date_str}, {datetime.now().year}'\n",
    "                date_format = '%a, %b %d, %Y' if \"May\" in date_str else '%a, %b. %d, %Y'\n",
    "                parsed_date = datetime.strptime(date_str, date_format)\n",
    "\n",
    "            # Append to the result lists\n",
    "            all_titles.append(title_text)\n",
    "            all_dates.append(parsed_date)\n",
    "        \n",
    "        page_num += 1\n",
    "\n",
    "    return all_titles, all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rithik/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "# Load the FinBERT model and tokenizer from Hugging Face\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Set up the pipeline for sentiment analysis\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, top_k=None, device=0)\n",
    "\n",
    "def analyze_sentiment(titles: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of article titles using FinBERT and returns cumulative sentiment scores.\n",
    "\n",
    "    Parameters:\n",
    "        titles (list): List of article titles.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of cumulative sentiment scores.\n",
    "    \"\"\"\n",
    "    cumulative_scores = []\n",
    "\n",
    "    for title in titles:\n",
    "        result = nlp(title)[0]  # Get sentiment scores for the title\n",
    "\n",
    "        # Extract individual scores\n",
    "        scores = {entry['label'].lower(): entry['score'] for entry in result}\n",
    "        positive = scores.get('positive', 0)\n",
    "        negative = scores.get('negative', 0)\n",
    "\n",
    "        # Calculate cumulative sentiment\n",
    "        cumulative_score = positive - negative\n",
    "\n",
    "        # Append to cumulative scores list\n",
    "        cumulative_scores.append(cumulative_score)\n",
    "\n",
    "    return np.array(cumulative_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def merge_sentiment(\n",
    "    date_list: list, \n",
    "    sent_arr: np.ndarray, \n",
    "    index_dates: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Merges sentiment scores by date, averaging multiple scores per day. \n",
    "    If a date has no articles, it fills the sentiment with 0.\n",
    "\n",
    "    Parameters:\n",
    "        date_list (list): List of datetime objects representing article publication dates.\n",
    "        sent_arr (np.ndarray): Array of sentiment scores corresponding to the article dates.\n",
    "        index_dates (np.ndarray): Array of indexed dates from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of average daily sentiment scores aligned with the index dates.\n",
    "    \"\"\"\n",
    "    # Initialize an array for daily sentiment scores aligned with the index dates\n",
    "    daily_sentiment = np.zeros(len(index_dates))\n",
    "    \n",
    "    # Create a dictionary to group sentiment scores by date\n",
    "    sentiment_by_date = {}\n",
    "    for date, score in zip(date_list, sent_arr):\n",
    "        if date not in sentiment_by_date:\n",
    "            sentiment_by_date[date] = []\n",
    "        sentiment_by_date[date].append(score)\n",
    "        \n",
    "    # Iterate over the dates and compute the average sentiment for each\n",
    "    for idx, current_date in enumerate(index_dates):\n",
    "        if current_date in sentiment_by_date:\n",
    "            # Calculate the average sentiment for the current date\n",
    "            daily_sentiment[idx] = np.mean(sentiment_by_date[current_date])\n",
    "\n",
    "\n",
    "    return daily_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m last_date \u001b[38;5;241m=\u001b[39m stock_ohlc\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get latest news articles and add sentiment scores\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m article_titles, dates  \u001b[38;5;241m=\u001b[39m \u001b[43mget_seekingalpha_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m sentiment_arr \u001b[38;5;241m=\u001b[39m analyze_sentiment(article_titles)\n\u001b[1;32m     31\u001b[0m sentiment_aligned \u001b[38;5;241m=\u001b[39m merge_sentiment(dates, sentiment_arr, stock_ohlc\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_pydatetime())\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mget_seekingalpha_articles\u001b[0;34m(ticker, start_date, end_date)\u001b[0m\n\u001b[1;32m     46\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Allow some time for the page to load\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[1;32m     52\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "stock = 'AMZN'\n",
    "period = '2y'\n",
    "interval = '1d'\n",
    "\n",
    "# Fetch stock data\n",
    "stock_ohlc = get_stock_data(stock, period, interval)\n",
    "\n",
    "# Calculate log returns\n",
    "log_returns_arr = calculate_log_returns(stock_ohlc['Close'])\n",
    "\n",
    "# Calculate technical indicators (RSI, EMA, VWAP, etc.)\n",
    "technical_indicators_dict = calculate_technical_indicators(stock_ohlc)\n",
    "\n",
    "# Add calculated fields to stock dataframe\n",
    "stock_df = stock_ohlc\n",
    "stock_df['log_returns'] = log_returns_arr\n",
    "stock_df['rsi'] = technical_indicators_dict['rsi']\n",
    "stock_df['ema'] = technical_indicators_dict['ema']\n",
    "stock_df['vwap'] = technical_indicators_dict['vwap']\n",
    "stock_df['adx'] = technical_indicators_dict['adx']\n",
    "stock_df['normalized_volume'] = technical_indicators_dict['normalized_volume']\n",
    "\n",
    "# Convert date to string for the article collection\n",
    "first_date = stock_ohlc.index.min()\n",
    "last_date = stock_ohlc.index.max()\n",
    "\n",
    "# Get latest news articles and add sentiment scores\n",
    "article_titles, dates  = get_seekingalpha_articles(stock, first_date, last_date)\n",
    "sentiment_arr = analyze_sentiment(article_titles)\n",
    "sentiment_aligned = merge_sentiment(dates, sentiment_arr, stock_ohlc.index.to_pydatetime())\n",
    "\n",
    "# Add the sentiment array to the stock DataFrame\n",
    "stock_df['sentiment_score'] = sentiment_aligned\n",
    "\n",
    "# Drop Nan values from DF\n",
    "stock_df = stock_df.dropna()\n",
    "\n",
    "# Display the final dataframe\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of log returns\n",
    "plt.hist(log_returns_arr, bins=80, edgecolor='black')\n",
    "plt.title('Distribution of Log Returns')\n",
    "plt.xlabel('Log Returns')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bimodal distribution observed in the log returns suggests volatility clustering, where the market exhibits frequent up or down movements, with fewer instances of no change. This non-linear behavior makes it challenging to use such features effectively in linear regression, as the model assumes a linear relationship between variables. The two peaks around zero highlight the market’s preference to shift rather than remain flat, reflecting non-stationary behavior. Also the data is shifted to the right slightly because stocks in general return positive returns over long time frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "df_corr = stock_df[[\"log_returns\", \"Volume\", \"rsi\", \"ema\", \"vwap\", \"adx\", \"sentiment_score\"]]\n",
    "correlations = df_corr.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table provides the correlations between closeing price for the day, volume of shares trades, technical indicators (like rsi, ema, vwap, adx), and sentiment score for each day the Amazon stock is traded. Based on these correlations, we can determine which variables have the strongest relationships, which gives us a better understanding of the data when analyzing. \n",
    "\n",
    "**Notable Relationships:**\n",
    "1. **Close and EMA (exponential moving average):** The high correlation of 0.904341 suggests that EMA can be a valuable indicator in predicting the closing price of Amazon's stock. This relationship makes sense the EMA is calulated using data from historical prices. This allows investors to confidently use the EMA as part of strategies to predict price movements, identify trends, and make informed decisions on entry and exit points for their trades.  \n",
    "2. **EMA and VWAP (volume weighted average price):** This relationship has a strong positive correlation of 0.907945. EMA tracks trends purely based on price, while vwap also takes volume of shares into consideration. This strong relationship suggests that price movements are well supported by trading volumes. This signals to investors good times to buy/sell a stock.    \n",
    "3. **VWAP and ADX (average directional index):** There is a moderate inverse relationship between these two factors, suggested by a correlation of -0.464629. ADX is used to measure the strength of a trend. As the VWAP increases, the ADX tends to decrease, which makes sense because with a higher trading volume comes greater uncertainty and prices may be more scattered.\n",
    "\n",
    "The correlation between technichal indicators and sentiment scores were also an area of interest for our group, however there does not appaer to be a strong relationship between any. Therefore, this tells us that the sentiment of recent news articles many not have a significant impact on stock trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the candlestick chart\n",
    "fig = go.Figure(data=[go.Candlestick(\n",
    "    x=stock_df.index,\n",
    "    open=stock_df['Open'],\n",
    "    high=stock_df['High'],\n",
    "    low=stock_df['Low'],\n",
    "    close=stock_df['Close'],\n",
    "    name='OHLC'\n",
    ")])\n",
    "\n",
    "# Add the EMA line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=stock_df.index,\n",
    "    y=stock_df['ema'],\n",
    "    mode='lines',\n",
    "    line=dict(width=1.5),\n",
    "    name='EMA'\n",
    "))\n",
    "\n",
    "# Add VWAP line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=stock_df.index,\n",
    "    y=stock_df['vwap'],\n",
    "    mode='lines',\n",
    "    line=dict(width=1.5, dash='dash'),\n",
    "    name='VWAP'\n",
    "))\n",
    "\n",
    "# Add ADX line on the secondary y-axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=stock_df.index,\n",
    "    y=stock_df['adx'],\n",
    "    mode='lines',\n",
    "    line=dict(width=1.5, color='purple'),\n",
    "    name='ADX',\n",
    "    yaxis=\"y2\"\n",
    "))\n",
    "\n",
    "# Add RSI line on a secondary y-axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=stock_df.index,\n",
    "    y=stock_df['rsi'],\n",
    "    mode='lines',\n",
    "    line=dict(width=1.5, color='orange'),\n",
    "    name='RSI',\n",
    "    yaxis=\"y3\"\n",
    "))\n",
    "\n",
    "# Update layout for readability and add secondary y-axes\n",
    "fig.update_layout(\n",
    "    title=f'{stock} Stock Price with Technical Indicators',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Stock Price',\n",
    "    yaxis2=dict(\n",
    "        title=\"ADX\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\"\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title=\"RSI\",\n",
    "        anchor=\"free\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        position=0.95  \n",
    "    ),\n",
    "    legend=dict(x=0, y=1.15),\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This candlestick plot shows the trend of Amazon's stock price from December 2023 all the way to the most recent date Amazon's stock was traded. The hover feature shows the date, open, close, high, and low prices of a particular day, as well as it shows the indicators we have included to help predict the graph. Additionally, based on the green and red color scheme, we can see when Amazon's stock price increased/decreased as well as the range it was traded at for the day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.read_csv('amzn_2y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up the features (X) and target (y)\n",
    "X_pd = stock_df[['rsi', 'ema', 'vwap', 'adx', 'normalized_volume', 'sentiment_score']]\n",
    "y_pd = np.where(stock_df[['log_returns']] >= 0, 1, -1)\n",
    "\n",
    "X = stock_df[['rsi', 'ema', 'vwap', 'adx', 'normalized_volume', 'sentiment_score']].to_numpy()\n",
    "y = np.where(stock_df[['log_returns']].to_numpy() >= 0, 1, -1)\n",
    "\n",
    "# Get the number of samples\n",
    "n = stock_df.shape[0]\n",
    "\n",
    "# Calculate the indices for the splits\n",
    "train_end = int(0.65 * n)  # 65% for training\n",
    "cv_end = int(0.80 * n)     # Next 15% for CV\n",
    "\n",
    "# Split the data\n",
    "X_train = X[:train_end]\n",
    "X_cv = X[train_end:cv_end]\n",
    "X_test = X[cv_end:]\n",
    "\n",
    "y_train = y[:train_end]\n",
    "y_cv = y[train_end:cv_end]\n",
    "y_test = y[cv_end:]\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cv_scaled = scaler.transform(X_cv)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Combine training and cross-validation sets\n",
    "X_combined = X_pd[:cv_end]\n",
    "y_combined = y_pd[:cv_end]\n",
    "\n",
    "# List of features to combine with sentiment_score\n",
    "features = ['rsi', 'ema', 'vwap', 'adx', 'normalized_volume']\n",
    "sentiment_feature = 'sentiment_score'\n",
    "target = 'log_returns'  # Target variable\n",
    "\n",
    "# Plot combined scatter plots\n",
    "def plot_combined_scatter(X, y, features, sentiment_feature):\n",
    "    for feature in features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Scatter plot for combined data\n",
    "        scatter = plt.scatter(\n",
    "            X[feature],\n",
    "            X[sentiment_feature],\n",
    "            c=y.flatten(),  # Ensure y is properly flattened for coloring\n",
    "            cmap='coolwarm',\n",
    "            alpha=0.7,\n",
    "            edgecolors='k'\n",
    "        )\n",
    "        \n",
    "        # Set labels and title\n",
    "        plt.title(f'{feature} vs {sentiment_feature} (Training + CV Set)')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(sentiment_feature)\n",
    "\n",
    "        # Add color bar\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Log Returns')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Call the function to create scatter plots\n",
    "plot_combined_scatter(X_combined, y_combined, features, sentiment_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming y is a numpy array with values -1 and 1\n",
    "counts = Counter(y_combined.flatten())\n",
    "\n",
    "# Extract the number of 1's and -1's\n",
    "num_ones = counts[1]\n",
    "num_neg_ones = counts[-1]\n",
    "\n",
    "print(f'Niave Guess Accuracy: {max(num_ones, num_neg_ones)/(num_ones+num_neg_ones)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing these plots I noticed that the rsi seemed like a pretty good feature to use as when it was higher the next day seemed to be more likely to be a win. This would imply that as a stock goes up it is more likely to go up in the future. The sentiment score does not seem as good as hypothesized even though it is an exteranl source that is not based on the underlying price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_perceptron(X, y, w, alpha = 1, max_iter = None):\n",
    "    \"\"\"\n",
    "    Implements the perceptron algorithm to update the weight vector `w` based on the given data.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): A 2D array where rows are data points and columns are features (including bias column of 1s).\n",
    "    y (numpy.ndarray): A 1D array of labels (-1 or 1) corresponding to each row in `X`.\n",
    "    w (numpy.ndarray): Initial weight vector with the same number of dimensions as the columns of `X`.\n",
    "    alpha (float): Learning rate, with a default value of 1.\n",
    "    max_iter (int or None): Maximum number of iterations. If `None`, the algorithm will run until convergence.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The final weight vector `w` after applying the perceptron algorithm.\n",
    "    \"\"\"\n",
    "    # Don't forget your docstring!\n",
    "\n",
    "    # I will set up the key parameters of the function and then the while loop\n",
    "    # you are responsible for the rest\n",
    "    runalg = True\n",
    "    i = 0\n",
    "    iter = 0\n",
    "\n",
    "    while runalg:\n",
    "\n",
    "        # for the current i, make the prediction\n",
    "        y_hat = 1 if (np.dot(X[i], w)) > 0 else -1\n",
    "        # check if it is correct\n",
    "        if y_hat != y[i]:\n",
    "            # if not, update w\n",
    "            w = w + alpha * y[i] * X[i]\n",
    "        # add one to i\n",
    "        i += 1\n",
    "        # if you've just updated the last i (the last observation in the data), add one to iter\n",
    "        if i == X.shape[0]:\n",
    "            i = 0\n",
    "            iter += 1\n",
    "        # if you've set a max_iter, and if you've REACHED the max_iter, set runalg = False, print w and iter, and break\n",
    "        if max_iter and iter >= max_iter:\n",
    "            runalg = False\n",
    "            print(f\"Final weights: {w}, Iterations: {iter}\")\n",
    "            break\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add a bias term (column of 1s) to the features\n",
    "X_train_bias = np.hstack((X_train_scaled, np.ones((X_train_scaled.shape[0], 1))))\n",
    "X_cv_bias = np.hstack((X_cv_scaled, np.ones((X_cv_scaled.shape[0], 1))))\n",
    "\n",
    "# Ensure labels are in {-1, 1}\n",
    "y_train_adjusted = np.where(y_train > 0, 1, -1)\n",
    "y_cv_adjusted = np.where(y_cv > 0, 1, -1)\n",
    "\n",
    "# Initialize weight vector (same length as number of columns in X_train_bias)\n",
    "w_initial = np.zeros(X_train_bias.shape[1])\n",
    "\n",
    "# Train the perceptron\n",
    "w_final = linear_perceptron(X_train_bias, y_train_adjusted, w_initial, alpha=1, max_iter=1000)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = np.sign(np.dot(X_train_bias, w_final))\n",
    "\n",
    "# Make predictions on the cross-validation set\n",
    "y_cv_pred = np.sign(np.dot(X_cv_bias, w_final))\n",
    "\n",
    "# Compute training and cross-validation accuracies\n",
    "train_accuracy = np.mean(y_train_pred == y_train_adjusted)\n",
    "cv_accuracy = np.mean(y_cv_pred == y_cv_adjusted)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Cross-Validation Accuracy: {cv_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above the Linear Perceptron model was able to reach 51% accuracy, which is similar to guessing. This shows that this model struggles to learn patterns. When we tried the Cross-Validation we got an accurracy of 54% which is sligtly better than Linear Perceptron. Additionally, the algorithm reached the maximum number of iterations (1000) without converging, implying that the dataset is likely not linearly separable. In such cases, the Perceptron cannot find a decision boundary that perfectly separates the classes.\n",
    "\n",
    "Due to the performance being really poorly, that suggests that we should not relly it's model. The final weight vector reflects the learned coefficients for each feature and the bias term, but these weights fail to establish a reliable decision boundary. The underperformance is likely due to non-linear relationships in the data, class imbalance, and a lack of feature interactions, which the Perceptron cannot adequately handle. The inability of the model to outperform a naive guess highlights its limitations in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning, UndefinedMetricWarning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=4, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = rf_classifier.predict(X_train_scaled)\n",
    "\n",
    "# Predict on the cross-validation set\n",
    "y_cv_pred = rf_classifier.predict(X_cv_scaled)\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "\n",
    "# Evaluate the model's performance on the cross-validation set\n",
    "cv_accuracy = accuracy_score(y_cv, y_cv_pred)\n",
    "print(f\"Cross-validation Accuracy: {cv_accuracy:.2f}\")\n",
    "\n",
    "# Print detailed classification metrics for training set\n",
    "print(\"Classification Report (Training Set):\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "# Print detailed classification metrics for cross-validation set\n",
    "print(\"Classification Report (Cross-Validation):\")\n",
    "print(classification_report(y_cv, y_cv_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model with 4 estimators shows a high train accuracy of 90%, meaning that the model performed well on the training data. In the case of stock-down days (-1), it has a precision of 0.85 and a recall of 0.95-that is, it can identify 95% of the actual stock-down days correctly with a very high confidence level in the prediction for this class. For the days of stock-up, for example, its precision amounts to 0.95 while its recall equates to 0.86, representing that the model can correctly identify 86% of real stock-up days with quite strong confidence. These finally yield balanced F1-scores for both classes (0.90 each) and assure that the model has duly learned from this training set.\n",
    "\n",
    "On the cross-validation set, the accuracy drops to 61%, indicating that this model generalizes poorly to new, unseen data. For class stock-down days (-1), the precision is 0.73, which means there is reasonable confidence in the model when it predicts this class, but the recall drops to 0.25, meaning only 25% of actual stock-down days are identified by the model. This results in a low F1-score for the -1 class, amounting to 0.37. In contrast, the model performs better for stock-up days (1), achieving a precision of 0.59 and a recall of 0.92, correctly identifying 92% of stock-up days while maintaining moderate confidence in its predictions. The F1-score for the 1 class is 0.72, reflecting better performance compared to the -1 class.\n",
    "\n",
    "Overall, the model is performing very well on the training set but is overfitting, as can be seen by the difference in performance between the training and cross-validation sets. The cross-validation results indicate a bias towards predicting stock-up days (1) with higher recall for this class and lower recall for stock-down days (-1). In the pursuit of better generalization and balancing the predictions, increasing the number of estimators, tuning hyperparameters with regard to max_depth and min_samples_split, or further addressing class imbalance through techniques such as oversampling the minority class or further adjustments of class_weight could help. This would enable the model to balance its predictions as well as generalize effectively to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying models like the Linear Perceptron or Random Forest to stock market predictions, a significant ethical concern is data bias and fairness. These models are trained on historical stock data, which may inadvertently reflect systemic biases, such as favoring certain sectors, companies, or periods of economic growth while neglecting others. For example, a Random Forest trained on data predominantly from a bullish market might predict upward trends more often, disadvantaging strategies that rely on balanced or bearish predictions. This raises fairness concerns, as the model's outputs might reinforce existing inequities in the market or mislead less informed traders.\n",
    "\n",
    "Another critical issue is transparency and accountability, especially with more complex models like Random Forests. While the Linear Perceptron provides clear decision boundaries, the Random Forest operates as a \"black box,\" making it difficult for users to understand how predictions are made. In stock trading, this lack of interpretability can lead to distrust in automated systems, especially when financial losses occur. Traders relying on these models may not fully understand the risks, potentially leading to over-reliance on predictions without critical oversight.\n",
    "\n",
    "Finally, the issue of accessibility and inequality is particularly relevant. Developing and deploying these models requires access to large datasets, computational resources, and domain expertise—resources often concentrated within institutional investors or well-funded individuals. Smaller investors are typically excluded from leveraging such tools, exacerbating existing wealth disparities. Without efforts to democratize these technologies, such as providing affordable tools or educational resources, these predictive models could widen the gap between institutional and retail traders, creating a less equitable financial landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert `log_returns` to a binary classification target: 1 for positive, 0 for negative\n",
    "stock_df['log_returns_binary'] = (stock_df['log_returns'] > 0).astype(int)\n",
    "\n",
    "# Select features and target\n",
    "features = ['rsi', 'ema', 'vwap', 'adx', 'normalized_volume', 'sentiment_score']\n",
    "X = stock_df[features]\n",
    "y = stock_df['log_returns_binary']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 15))\n",
    "plot_tree(\n",
    "    clf, \n",
    "    feature_names=features, \n",
    "    class_names=['Negative Return', 'Positive Return'], \n",
    "    filled=True, \n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Decision Tree for Predicting Log Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
