{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase II: Data Curation, Exploratory Analysis and Plotting\n",
    "## Stock Market Predictor\n",
    "\n",
    "### Names: Diego Cicotoste, Ariv Ahuja, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "How does the stock market work? how can you predict the stock market? what tools can you use? The stock market can seem complex and unpredictable, some would even say gambeling. One of the hardest challenges is making educated or informed decisions. The goal of this project is to tackle the uncertainty and help, stock traders make better decision on wether a stock is tradable or not. Wether to buy or sell. I would use past historical trends to make educated predictions on how the stock market would react."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Retrieval**  \n",
    "- We used the **`yfinance` API** to retrieve daily historical **Open, High, Low, Close (OHLC)** prices and **volume data** for **S&P 500 stocks**, focusing on **Amazon (AMZN)** for the **past year**.\n",
    "- The retrieved data includes essential market metrics that will serve as the foundation for feature engineering.\n",
    "- We used **`NewsAPI`** to get article data on the stock\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Data Cleaning and Processing**\n",
    "\n",
    "#### **Handling Missing Data**  \n",
    "- No data was missing after inspection\n",
    "\n",
    "#### **Feature Engineering: Technical Indicators**  \n",
    "We calculated several key **technical indicators** to enrich the dataset:\n",
    "  - **RSI (Relative Strength Index)**: Momentum indicator over 14 days.\n",
    "  - **VWAP (Volume Weighted Average Price)**: Measures the average trading price weighted by volume.\n",
    "  - **EMA (Exponential Moving Average)**: Captures the smoothed trend over 20 days.\n",
    "  - **ADX (Average Directional Index)**: Quantifies trend strength.\n",
    "\n",
    "#### **More Features: Sentiment Analysis from News Articles**  \n",
    "- We fetched relevant **news articles** using **NewsAPI** for the same period as the stock data.\n",
    "- **VADER Sentiment Analysis** was used to calculate **compound sentiment scores** for each article.\n",
    "- Sentiment scores were **aggregated by date** to align with the stock OHLC data.\n",
    "\n",
    "#### **Data Alignment and Merging**  \n",
    "- We ensured **alignment** between **OHLC data, technical indicators, log returns, and sentiment scores** using date-based indices.\n",
    "- The combined DataFrame was prepared, with all relevant features available for further analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Visualization of the Cleaned Data**\n",
    "\n",
    "We visualized the **cleaned and processed dataset** to understand key trends and patterns:\n",
    "\n",
    "1. **Price Trends and Indicators**:\n",
    "   - **OHLC Candlestick Plots**: Show stock price movements.\n",
    "   - **Overlaying VWAP and EMA**: To track trends and identify support/resistance levels.\n",
    "   - **RSI and ADX Line Plots**: Visualize momentum and trend strength over time.\n",
    "\n",
    "2. **Volume Analysis**:\n",
    "   - **Normalized Volume**: Visualized to detect significant changes in trading activity.\n",
    "\n",
    "3. **Sentiment Trends**:\n",
    "   - **Sentiment Score Line Chart**: Displays how public sentiment fluctuates over time.\n",
    "   - **Overlay of Sentiment with Stock Price**: To observe correlations between sentiment and price movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_stock_data(symbol: str, period: str, interval: str = '1d') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve stock price data for a given symbol, time period, and interval.\n",
    "    Returns the stock prices as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): The ticker symbol of the stock (e.g., 'AAPL').\n",
    "        period (str): The period to retrieve data (e.g., '1y', '6mo', '5d').\n",
    "        interval (str): The data interval (e.g., '1d', '1wk', '1mo').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing historical stock prices.\n",
    "    \"\"\"\n",
    "    # Fetch data from Yahoo Finance\n",
    "    stock_data = yf.download(symbol, period=period, interval=interval)\n",
    "\n",
    "    if stock_data.empty:\n",
    "        print(f\"No data found for {symbol}.\")\n",
    "        return None\n",
    "\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_log_returns(close: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the log returns from the close prices.\n",
    "    \n",
    "    Parameters:\n",
    "        close (np.ndarray): Array of closing prices.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of log returns.\n",
    "    \"\"\"\n",
    "    log_returns = np.log(close / close.shift(1))\n",
    "    return log_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "\n",
    "def calculate_technical_indicators(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate technical indicators and return them as NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing historical stock prices.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with technical indicators as NumPy arrays.\n",
    "    \"\"\"\n",
    "    indicators = {}\n",
    "\n",
    "    # Calculate RSI (Relative Strength Index)\n",
    "    indicators['rsi'] = ta.rsi(df['Close'], length=14).to_numpy()\n",
    "\n",
    "    # Calculate 20-day Exponential Moving Average (EMA)\n",
    "    indicators['ema_20'] = ta.ema(df['Close'], length=20).to_numpy()\n",
    "\n",
    "    # Calculate ADX (Average Directional Index)\n",
    "    adx_df = ta.adx(df['High'], df['Low'], df['Close'], length=14)\n",
    "    indicators['adx'] = adx_df['ADX_14'].to_numpy()\n",
    "\n",
    "    # Calculate VWAP (Volume Weighted Average Price)\n",
    "    vwap_series = ta.vwap(df['High'], df['Low'], df['Close'], df['Volume'])\n",
    "    indicators['vwap'] = vwap_series.to_numpy()\n",
    "\n",
    "    # Calculate normalized volume\n",
    "    indicators['normalized_volume'] = (df['Volume'] / df['Volume'].rolling(window=20).mean()).to_numpy()\n",
    "\n",
    "    return indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "\n",
    "def get_news_articles(stock_symbol: str, api_key: str, from_date: str, to_date: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve news articles related to the given stock symbol along with their publication dates.\n",
    "    \"\"\"\n",
    "    # Construct the NewsAPI request URL\n",
    "    url = (\n",
    "        f\"https://newsapi.org/v2/everything?q={stock_symbol}&from={from_date}&to={to_date}&\"\n",
    "        f\"sortBy=publishedAt&language=en&apiKey={api_key}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Make the API request with timeout\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        news_data = response.json()\n",
    "\n",
    "        # Check if the response contains articles\n",
    "        if 'articles' not in news_data or len(news_data['articles']) == 0:\n",
    "            print(\"No articles found for the given date range or stock symbol\")\n",
    "            return []\n",
    "\n",
    "        # Extract article details (title, description, date)\n",
    "        articles = [\n",
    "            {\n",
    "                'title': article['title'],\n",
    "                'description': article.get('description', ''),\n",
    "                'publishedAt': article['publishedAt']\n",
    "            }\n",
    "            for article in news_data['articles']\n",
    "        ]\n",
    "\n",
    "        return articles\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error: {e}\")\n",
    "        return []\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Invalid response format or unable to parse JSON\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def add_sentiment_scores_to_articles(articles: list) -> list:\n",
    "    \"\"\"\n",
    "    Add compound sentiment scores to each article dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        articles (list): List of dictionaries containing article titles, descriptions, and publication dates.\n",
    "\n",
    "    Returns:\n",
    "        list: The original list of dictionaries with added 'compound_score' keys.\n",
    "    \"\"\"\n",
    "    # Initialize VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Add sentiment scores to each article\n",
    "    for article in articles:\n",
    "        title = article['title'] or \"\"\n",
    "        description = article.get('description', \"\")\n",
    "        text = f\"{title} {description}\"\n",
    "\n",
    "        # Get the compound sentiment score and add it to the article dictionary\n",
    "        article['compound_score'] = analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_to_sentiment_arr(articles: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a list of articles with sentiment scores into a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        articles (list): List of dictionaries with 'publishedAt' and 'compound_score' keys.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of average compound sentiment scores grouped by whole dates.\n",
    "    \"\"\"\n",
    "    # Prepare data for the DataFrame\n",
    "    sentiment_data = [\n",
    "        {'date': article['publishedAt'].split('T')[0], 'compound_score': article['compound_score']}\n",
    "        for article in articles\n",
    "    ]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    sentiment_df = pd.DataFrame(sentiment_data)\n",
    "\n",
    "    # Convert 'date' to datetime format and group by date to calculate average sentiment\n",
    "    sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "    aggregated_sentiment = sentiment_df.groupby(sentiment_df['date'].dt.date).mean()\n",
    "\n",
    "    # Return the average sentiment scores as a NumPy array\n",
    "    return aggregated_sentiment['compound_score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13e6a526771f44a298c0463c9b01b913\n",
      "Network error: 426 Client Error: Upgrade Required for url: https://newsapi.org/v2/everything?q=AMZN&from=2023-10-19&to=2024-10-18&sortBy=publishedAt&language=en&apiKey=13e6a526771f44a298c0463c9b01b913\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/diegocicotoste/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get news articles and sentiment\u001b[39;00m\n\u001b[1;32m     20\u001b[0m article_list \u001b[38;5;241m=\u001b[39m get_news_articles(stock, news_api_key, first_date, last_date)\n\u001b[0;32m---> 21\u001b[0m article_list_sent \u001b[38;5;241m=\u001b[39m \u001b[43madd_sentiment_scores_to_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m aggregated_sentiment_arr \u001b[38;5;241m=\u001b[39m articles_to_sentiment_arr(article_list_sent)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Add calculated fields to stock dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 17\u001b[0m, in \u001b[0;36madd_sentiment_scores_to_articles\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mAdd compound sentiment scores to each article dictionary.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    list: The original list of dictionaries with added 'compound_score' keys.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize VADER sentiment analyzer\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Add sentiment scores to each article\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:836\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    839\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:962\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    959\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/diegocicotoste/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from my_secrets import news_api_key\n",
    "\n",
    "# Example usage\n",
    "stock = 'AMZN'\n",
    "period = '1y'\n",
    "interval = '1d'\n",
    "\n",
    "print(news_api_key)\n",
    "\n",
    "# Fetch stock data\n",
    "stock_ohlc = get_stock_data(stock, period, interval)\n",
    "log_returns_arr = calculate_log_returns(stock_ohlc['Close'])\n",
    "technical_indicators_dict = calculate_technical_indicators(stock_ohlc)\n",
    "\n",
    "# Convert date to string for the news API\n",
    "first_date = stock_ohlc.index.min().strftime('%Y-%m-%d')\n",
    "last_date = stock_ohlc.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "# Get news articles and sentiment\n",
    "article_list = get_news_articles(stock, news_api_key, first_date, last_date)\n",
    "article_list_sent = add_sentiment_scores_to_articles(article_list)\n",
    "aggregated_sentiment_arr = articles_to_sentiment_arr(article_list_sent)\n",
    "\n",
    "# Add calculated fields to stock dataframe\n",
    "stock_df = stock_ohlc\n",
    "stock_df['log_returns'] = log_returns_arr\n",
    "stock_df['rsi'] = technical_indicators_dict['rsi']\n",
    "stock_df['ema_20'] = technical_indicators_dict['ema_20']\n",
    "stock_df['vwap'] = technical_indicators_dict['vwap']\n",
    "stock_df['normalized_volume'] = technical_indicators_dict['normalized_volume']\n",
    "stock_df['sentiment_score'] = aggregated_sentiment_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
